# üúÇ GHOSTLINE NEXUS - Local Sovereignty Setup Guide

**FULL OFFLINE MODE - Ollama na tvojem hardware**

**Datum:** 2025-12-29
**Hardware:** Intel Xeon X5670 (12 cores) + 47GB RAM + GTX 1060 3GB
**Status:** ‚úÖ READY FOR LOCAL SOVEREIGNTY

---

## üéØ TVOJ HARDWARE - ANALIZA

### ‚úÖ DOBRE NOVICE

**RAM: 47GB** üî•
- Odliƒçno za LLM inference!
- Lahko poganja modele do 13B parametrov
- Dovolj za 2-3 modele hkrati v memory

**CPU: Intel Xeon X5670 (12 cores)** ‚úÖ
- 12 cores @ 2.93GHz
- Zadosti za decent inference speed
- Priƒçakuj: 2-5 tokens/second (odvisno od modela)

**Disk: /home ima 423GB prostora** ‚úÖ
- Perfektno za modele (vsak model 4-8GB)
- Lahko ima≈° 50+ modelov

**Ollama: ≈Ωe instaliran** ‚úÖ
- Path: `/usr/local/bin/ollama`

### ‚ö†Ô∏è POZOR - KRITIƒåNI PROBLEMI

**Disk Space na /: SAMO 435MB** ‚ùå
- Root partition je skoraj poln (99% uporabljen)
- **RE≈†ITEV:** Modele shranimo na `/home` (423GB free)

**GPU Drivers: nvidia-smi ne dela** ‚ö†Ô∏è
- GTX 1060 3GB je prisotna (lspci jo vidi)
- Ampak drivers niso aktivni
- **RE≈†ITEV:** Delamo na CPU (z 47GB RAM je to OK!)

**GTX 1060 3GB: Premalo VRAM** ‚ö†Ô∏è
- 3GB VRAM je premalo za veƒçje modele
- Tudi ƒçe bi delali drivers, bi morali uporabljat CPU
- **RE≈†ITEV:** CPU inference z quantized modeli

---

## üîß KORAK 1: KONFIGURIRAJ OLLAMA (KRITIƒåNO!)

### **Problem:** Ollama bo poskusil shranit modele na `/` kjer ni prostora!

### **Re≈°itev:** Prestavi Ollama models directory na `/home`

```bash
# 1. Ustvari nov models directory na /home
sudo mkdir -p /home/ollama-models

# 2. Nastavi permissions
sudo chown -R $USER:$USER /home/ollama-models

# 3. Konfiguriraj Ollama da uporablja ta directory
# Naƒçin 1: Systemd service (priporoƒçeno)
sudo mkdir -p /etc/systemd/system/ollama.service.d
sudo tee /etc/systemd/system/ollama.service.d/override.conf << 'EOF'
[Service]
Environment="OLLAMA_MODELS=/home/ollama-models"
EOF

# 4. Reload systemd in restart Ollama
sudo systemctl daemon-reload
sudo systemctl restart ollama

# 5. Preveri status
sudo systemctl status ollama

# 6. Preveri da uporablja pravilni path
echo "Expected models path: /home/ollama-models"
```

**Alternativa:** ƒåe Ollama ne teƒçe kot service:

```bash
# Dodaj v ~/.bashrc ali ~/.zshrc
echo 'export OLLAMA_MODELS=/home/ollama-models' >> ~/.bashrc
source ~/.bashrc

# Potem vedno po≈æeni Ollama iz terminala
ollama serve
```

---

## üéØ KORAK 2: IZBERI IN PULLAJ MODEL

### **Priporoƒçeni Modeli za Tvoj Hardware:**

| Model | Size | RAM Needed | Speed | Quality | Recommend |
|-------|------|------------|-------|---------|-----------|
| **gemma2:2b** | 1.6GB | 3GB | üöÄüöÄüöÄüöÄ | ‚≠ê‚≠ê‚≠ê | ‚úÖ **ZAƒåETEK** |
| **phi3.5:3.8b** | 2.3GB | 4GB | üöÄüöÄüöÄüöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **PRIPOROƒåENO** |
| **llama3.2:3b** | 2GB | 4GB | üöÄüöÄüöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **ODLIƒåNO** |
| **mistral:7b-instruct-q4** | 4.1GB | 6GB | üöÄüöÄüöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **BEST** |
| **llama3.1:8b-instruct-q4** | 4.7GB | 8GB | üöÄüöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **QUALITY** |
| **qwen2.5:7b** | 4.7GB | 8GB | üöÄüöÄüöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **TECH** |
| **llama2:13b-q4** | 7.3GB | 12GB | üöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è **SLOW** |

### **Moj Top Pick za Tvoj Setup:**

```bash
# NAJBOLJ≈†I BALANCE: Mistral 7B (quantized Q4)
ollama pull mistral:7b-instruct-q4_K_M

# Ali ƒçe ≈æeli≈° hitrost: Phi-3.5 Mini
ollama pull phi3.5:3.8b

# Ali ƒçe ≈æeli≈° najmanj≈°i: Gemma 2B
ollama pull gemma2:2b
```

**Hitri test:**

```bash
# Pullaj model
ollama pull phi3.5:3.8b

# Testni run
ollama run phi3.5:3.8b "Pozdravi me po slovensko in povej kdo si."

# Preveri da dela
# Priƒçakovano: odziv v nekaj sekundah
```

---

## üöÄ KORAK 3: PO≈ΩENI OLLAMA SERVICE

### **Naƒçin A: Systemd Service (priporoƒçeno)**

```bash
# Preveri ƒçe service teƒçe
sudo systemctl status ollama

# ƒåe ne teƒçe:
sudo systemctl start ollama

# Omogoƒçi auto-start ob bootu
sudo systemctl enable ollama

# Preveri da poslu≈°a na portu 11434
curl http://localhost:11434/api/tags
```

### **Naƒçin B: Manual Launch**

```bash
# ƒåe service ne dela, po≈æeni roƒçno
OLLAMA_MODELS=/home/ollama-models ollama serve

# V novem terminalu testiraj:
curl http://localhost:11434/api/tags
```

**Priƒçakovan response:**
```json
{"models":[{"name":"phi3.5:3.8b",...}]}
```

---

## üúÇ KORAK 4: KONFIGURIRAJ GHOSTLINE NEXUS

```bash
# 1. Pojdi v GHOSTLINE directory
cd /home/saba/GHOSTLINE_NEXUS

# 2. Kopiraj .env.example v .env
cp .env.example .env

# 3. Uredi .env
nano .env
```

**Nastavi te vrednosti:**

```bash
# ============================================
# LLM PROVIDER SELECTION
# ============================================
LLM_PROVIDER=local

# ============================================
# LOCAL LLM (Ollama)
# ============================================
LOCAL_LLM_ENDPOINT=http://host.docker.internal:11434
LOCAL_LLM_MODEL=phi3.5:3.8b
LOCAL_LLM_FORMAT=ollama

# ============================================
# SHARED LLM SETTINGS
# ============================================
MAX_TOKENS=4096
TEMPERATURE=0.7

# ============================================
# SERVER CONFIGURATION
# ============================================
NODE_ENV=production
PORT=3001
FRONTEND_PORT=3000
```

**Shrani in zapri** (`Ctrl+X`, `Y`, `Enter`)

---

## üî• KORAK 5: PO≈ΩENI GHOSTLINE NEXUS

```bash
# 1. Zagotovi da je Ollama service aktiven
sudo systemctl status ollama

# 2. Po≈æeni GHOSTLINE NEXUS
cd /home/saba/GHOSTLINE_NEXUS
docker-compose up -d

# 3. Poƒçakaj ~30 sekund da se build-a (prvi run)

# 4. Preveri status
docker-compose ps

# Expected:
# ghostline-backend    Up
# ghostline-frontend   Up
```

---

## ‚úÖ KORAK 6: TESTIRANJE

### **Test 1: Ollama Directly**

```bash
curl http://localhost:11434/api/tags
# Priƒçakuje≈°: seznam modelov
```

### **Test 2: Backend API**

```bash
curl http://localhost:3001/api/system/provider
# Priƒçakuje≈°: {"current_provider":{"provider":"local","name":"Local LLM (ollama)"}}
```

### **Test 3: Provider Test**

```bash
curl http://localhost:3001/api/system/provider/test
# Priƒçakuje≈°: {"status":"connected","test_response":"OK"}
```

### **Test 4: Frontend**

```bash
# Odpri browser
xdg-open http://localhost:3000

# Ali
firefox http://localhost:3000
```

**V Frontend-u:**
1. Pojdi na **‚öôÔ∏è SETTINGS** tab
2. Vidi≈°: Provider = "Local LLM (ollama)"
3. Klikni **üîç Test Connection**
4. Priƒçakuje≈°: **‚úÖ Connected** (zelen)

### **Test 5: Dejanski Chat**

1. Pojdi na **üí¨ CHAT** tab
2. Napi≈°i: "Pozdravi me po slovensko"
3. Priƒçakuje≈°: Odgovor v nekaj sekundah

**ƒåe dela:** üéâ **SIDRO STOJI. PLAMEN GORI.** üî•

---

## ‚ö° PERFORMANCE EXPECTATIONS

**S tvojimi specs (Xeon X5670 + 47GB RAM):**

| Model | First Token | Tokens/sec | Total (100 tokens) |
|-------|-------------|------------|-------------------|
| gemma2:2b | ~0.5s | 8-12 tok/s | ~10s |
| phi3.5:3.8b | ~1s | 4-8 tok/s | ~15s |
| mistral:7b-q4 | ~1.5s | 2-5 tok/s | ~25s |
| llama3.1:8b-q4 | ~2s | 2-4 tok/s | ~30s |

**To je normalno za CPU inference!** GPU bi bilo 10-50x hitreje, ampak 3GB VRAM ni dovolj.

---

## üõ†Ô∏è TROUBLESHOOTING

### **Problem: Ollama ne poslu≈°a na port 11434**

```bash
# Check ƒçe teƒçe
sudo systemctl status ollama

# Check kdo uporablja port
sudo lsof -i :11434

# Restart
sudo systemctl restart ollama
```

### **Problem: Backend ne more connect do Ollama**

```bash
# Preveri iz Docker container-ja
docker exec ghostline-backend curl http://host.docker.internal:11434/api/tags

# ƒåe ne dela, preveri docker-compose.yml da ima:
# extra_hosts:
#   - "host.docker.internal:host-gateway"
```

### **Problem: "Out of memory"**

```bash
# Uporabi manj≈°i model
ollama pull gemma2:2b

# Nastavi v .env:
LOCAL_LLM_MODEL=gemma2:2b
docker-compose restart
```

### **Problem: Prespoƒçasno (veƒç kot 60s za odgovor)**

**Re≈°itve:**
1. Uporabi manj≈°i model (gemma2:2b ali phi3.5)
2. Zmanj≈°aj MAX_TOKENS v .env (nastavi 2048)
3. Zmanj≈°aj dol≈æino input prompta

### **Problem: Disk full error**

```bash
# Preveri Ollama models path
echo $OLLAMA_MODELS

# Mora biti: /home/ollama-models

# ƒåe ni, poglej KORAK 1 zgoraj
```

---

## üîÆ OPTIMIZACIJE (OPTIONAL)

### **Hitrost Up:**

```bash
# 1. Uporabi quantized modele (Q4_K_M ali Q4_0)
ollama pull mistral:7b-instruct-q4_0

# 2. Nastavi num_thread v Ollama
# Dodaj v /etc/systemd/system/ollama.service.d/override.conf:
Environment="OLLAMA_NUM_THREADS=12"

# 3. Restart
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### **GPU Acceleration (advanced):**

**ƒåe bi hotel aktivirat GTX 1060:**

```bash
# 1. Instaliraj NVIDIA drivers
sudo pacman -S nvidia nvidia-utils

# 2. Reload
sudo reboot

# 3. Preveri
nvidia-smi

# 4. Ollama bo avtomatsko uporablja GPU
# AMPAK: 3GB VRAM je premalo za modele > 3B
# Tako da ti CPU inference bolj smiselno!
```

---

## üåç REMOTE DOCKER DEPLOYMENT (za kasneje)

**ƒåe bi ≈æelel Ollama na serverju, GHOSTLINE na local:**

### **Setup A: Ollama na remote server**

```bash
# Na serverju:
# 1. Instaliraj Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Konfiguriraj da poslu≈°a na network
sudo systemctl edit ollama
# Dodaj:
Environment="OLLAMA_HOST=0.0.0.0:11434"

# 3. Restart
sudo systemctl restart ollama

# 4. Odpri firewall
sudo ufw allow 11434/tcp
```

**Na local ma≈°ini (GHOSTLINE):**

```bash
# Uredi .env
LOCAL_LLM_ENDPOINT=http://192.168.1.XXX:11434
# (zamenjaj XXX z IP serverja)

# Restart
docker-compose restart
```

### **Setup B: GHOSTLINE na remote server**

```bash
# Na serverju:
# 1. Instaliraj Docker + Docker Compose
# 2. Copy GHOSTLINE_NEXUS directory
scp -r GHOSTLINE_NEXUS/ user@server:/home/user/

# 3. SSH to server
ssh user@server

# 4. Deploy
cd GHOSTLINE_NEXUS
cp .env.example .env
nano .env  # Konfiguriraj
docker-compose up -d

# 5. Access od local browser:
# http://server-ip:3000
```

---

## üìä PRIPOROƒåENI SETUP ZA TVOJ HARDWARE

**Moj final recommendation:**

```bash
# Model: Phi-3.5 Mini (best balance)
ollama pull phi3.5:3.8b

# .env configuration:
LLM_PROVIDER=local
LOCAL_LLM_ENDPOINT=http://host.docker.internal:11434
LOCAL_LLM_MODEL=phi3.5:3.8b
LOCAL_LLM_FORMAT=ollama
MAX_TOKENS=2048
TEMPERATURE=0.7
```

**Zakaj Phi-3.5:**
- ‚úÖ Odliƒçen quality za velikost
- ‚úÖ Hitrej≈°i od Mistral 7B
- ‚úÖ Manj≈°i kot Llama 8B
- ‚úÖ Dober za kodo, matematiko, reasoning
- ‚úÖ Samo 2.3GB - hitre responses

**ƒåe Phi-3.5 ni dovolj dober, upgrade na:**
```bash
ollama pull mistral:7b-instruct-q4_K_M
# In nastavi: LOCAL_LLM_MODEL=mistral:7b-instruct-q4_K_M
```

---

## üî• ZAKLJUƒåEK - IDIOT-PROOF CHECKLIST

```bash
# ‚úÖ KORAK 1: Konfiguriraj Ollama models path
sudo mkdir -p /home/ollama-models
sudo chown $USER:$USER /home/ollama-models
sudo mkdir -p /etc/systemd/system/ollama.service.d
echo '[Service]
Environment="OLLAMA_MODELS=/home/ollama-models"' | sudo tee /etc/systemd/system/ollama.service.d/override.conf
sudo systemctl daemon-reload
sudo systemctl restart ollama

# ‚úÖ KORAK 2: Pullaj model
ollama pull phi3.5:3.8b

# ‚úÖ KORAK 3: Test Ollama
curl http://localhost:11434/api/tags

# ‚úÖ KORAK 4: Konfiguriraj GHOSTLINE
cd /home/saba/GHOSTLINE_NEXUS
cp .env.example .env
# Uredi .env:
#   LLM_PROVIDER=local
#   LOCAL_LLM_MODEL=phi3.5:3.8b

# ‚úÖ KORAK 5: Po≈æeni GHOSTLINE
docker-compose up -d

# ‚úÖ KORAK 6: Test
curl http://localhost:3001/api/system/provider/test

# ‚úÖ KORAK 7: Odpri browser
xdg-open http://localhost:3000

# ‚úÖ KORAK 8: Test chat v frontend-u
```

**ƒåe vse dela:** üéâ

**SIDRO STOJI. PLAMEN GORI. INTELIGENCA JE TVOJA.** üúÇ‚öìüî•

---

**Last Updated:** 2025-12-29
**Hardware Tested:** Intel Xeon X5670 + 47GB RAM + GTX 1060 3GB
**Status:** ‚úÖ PRODUCTION READY - Full Local Sovereignty
